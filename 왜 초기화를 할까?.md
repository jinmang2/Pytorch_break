```python

import torch
import torch.nn as nn


x = torch.randn(2, 64)
```

```python
# Case 1
model = nn.Linear(64, 10)
# model.weight = nn.Parameter(torch.zeros(10, 64))
# model.bias = nn.Parameter(torch.zeros(10,))

z = model(x)
logit = torch.sigmoid(z)

optimizer = torch.optim.Adam(model.parameters())
criterion = nn.BCELoss()
y_true = torch.zeros(logit.size())
bsz = y_true.size(0)
y_true[torch.arange(bsz), torch.LongTensor(bsz).random_(0,2)] = 1

loss = criterion(logit, y_true)
loss.backward()
optimizer.step()

model.weight, model.bias
```
```
(Parameter containing:
 tensor([[-2.2854e-02,  1.8702e-03, -3.6347e-02, -1.7398e-03,  3.1776e-02,
           8.9544e-02,  1.1363e-01,  1.2031e-01, -9.4285e-02,  1.1752e-01,
          -2.3778e-02,  5.1337e-03,  6.4341e-02, -1.2162e-01,  6.6711e-02,
          -8.3408e-02, -8.8270e-02,  7.3745e-02, -1.6841e-02, -5.1310e-02,
           3.6151e-02, -1.1480e-01, -7.8558e-02, -7.3974e-02, -5.7259e-02,
          -1.1513e-02, -8.9135e-02,  8.8435e-02,  1.0354e-02,  4.5301e-02,
          -2.0303e-02, -6.7405e-02,  1.1235e-01, -1.8435e-02,  1.6430e-02,
           1.1882e-01,  3.9544e-02,  5.0064e-02, -1.4800e-02, -8.3529e-02,
           2.1504e-02,  8.7840e-02, -1.1282e-01,  5.2362e-02,  7.0172e-05,
           3.0212e-02, -1.5285e-02,  8.4700e-02,  1.0986e-01,  6.7691e-02,
          -1.0840e-01, -4.0794e-02,  1.2539e-01,  7.2783e-02,  9.9480e-02,
          -1.0633e-02,  4.6112e-02,  2.5576e-02, -4.3259e-02,  1.0070e-01,
          -7.6192e-02,  5.0303e-02, -7.1609e-02, -1.0362e-01],
         [ 5.6086e-02,  3.8839e-02,  3.3244e-02, -1.0675e-01,  6.1362e-02,
           5.5305e-02, -4.0959e-02,  1.0370e-01,  8.7792e-02,  3.9377e-02,
           7.9600e-03,  4.7585e-03, -5.7136e-02, -3.2465e-02, -3.3348e-02,
          -7.1345e-02,  1.0837e-01, -7.5574e-02,  5.1637e-02,  3.2194e-02,
           8.9360e-02,  8.7120e-02,  8.7871e-02,  1.0213e-01, -1.2573e-01,
          -1.0896e-01,  8.6614e-02, -1.1649e-01, -3.3835e-02, -9.7458e-02,
           2.1466e-02, -6.9220e-02,  6.5674e-02,  8.7744e-02, -1.0374e-01,
           1.8463e-02,  7.6190e-02, -9.1256e-03, -1.1552e-01, -1.6109e-02,
          -8.4947e-03,  2.8541e-03, -1.0563e-01, -5.7558e-02,  9.6691e-02,
          -3.2150e-02, -6.1611e-02,  8.9042e-02, -4.4377e-02,  1.4587e-02,
           1.1152e-01, -6.6124e-02, -4.1683e-03, -4.0701e-02, -2.0116e-02,
           1.0404e-01, -2.0523e-02, -1.1238e-01,  9.2709e-03, -6.9238e-02,
          -1.2308e-01, -3.3558e-02, -1.1206e-01, -1.0788e-01],
         [-5.4928e-02, -4.9347e-03,  1.7856e-02,  7.7747e-03,  7.1037e-02,
           1.1221e-01, -8.1743e-02,  5.1312e-02,  9.2711e-02, -4.0710e-02,
          -1.1944e-01,  6.1878e-02,  1.9590e-02, -6.9643e-02,  5.4337e-02,
          -3.0445e-02, -1.1249e-04, -8.8275e-02, -8.0719e-02, -3.7874e-02,
          -7.5657e-02,  1.7929e-02, -3.8387e-02, -1.1006e-01, -1.2565e-02,
           4.6733e-03, -3.0582e-02, -6.5565e-02, -1.2463e-01,  6.7606e-02,
           3.1127e-02, -1.1164e-01,  2.7428e-02, -9.8866e-02, -6.3908e-02,
          -1.0989e-01, -6.4916e-02,  8.6798e-02,  1.4648e-02, -7.6413e-03,
          -2.5898e-02, -1.2142e-01, -1.1956e-02,  2.5191e-02, -5.1097e-02,
          -1.2283e-01, -1.1128e-01, -1.1641e-01,  7.6668e-02,  8.2501e-02,
          -2.7901e-02,  5.0725e-02,  8.7998e-02,  1.1026e-01, -7.1881e-03,
          -9.0794e-02, -8.3877e-02, -6.3767e-03, -8.0766e-02, -8.4598e-02,
           4.6634e-02,  8.9113e-02, -2.4613e-02, -4.9839e-02],
         [-3.7273e-04, -2.4555e-02, -4.5327e-02,  6.4342e-02, -5.1095e-02,
           5.5822e-02, -3.9948e-02, -5.1994e-02, -3.5389e-02,  1.1107e-01,
          -5.3482e-02, -2.3329e-02, -1.0872e-01,  9.3377e-02,  1.0220e-01,
           8.7293e-02,  1.2088e-01, -1.7263e-02, -2.4675e-02, -1.9845e-02,
          -1.1628e-01,  4.5313e-02,  4.3083e-02, -6.2138e-02,  3.0681e-02,
           1.2127e-01,  2.7282e-02,  4.7601e-02, -2.9823e-02,  5.0318e-02,
           7.5097e-03, -3.4820e-02, -6.6113e-02, -1.9207e-03, -2.9361e-03,
          -1.2369e-01, -1.0258e-01, -3.5175e-02,  1.7618e-02,  1.1283e-01,
          -1.0603e-01,  6.1256e-02,  2.4903e-02,  6.0464e-02, -5.0491e-02,
          -8.8285e-02,  2.6892e-02,  8.7236e-02,  1.0479e-01, -1.3133e-02,
           9.7186e-02,  1.1312e-01, -4.5413e-02,  6.6660e-02, -1.0397e-01,
           7.6939e-02,  1.0452e-01,  7.2921e-02,  7.3351e-03,  5.7916e-02,
          -8.4378e-02,  1.1998e-01, -2.0569e-02, -2.7177e-02],
         [-5.7667e-02,  7.7390e-03,  1.0043e-01,  6.1453e-02, -8.5754e-03,
           6.3667e-02,  4.7690e-02, -1.1224e-01,  6.7858e-02,  5.9833e-02,
           3.1933e-02, -8.4277e-02,  5.7854e-02, -9.2558e-02,  1.1993e-01,
          -4.4060e-02,  8.6762e-02, -9.4564e-02,  1.1914e-02,  1.0930e-01,
           1.2259e-01, -3.4430e-02, -4.5734e-03,  1.1316e-01, -6.8248e-02,
           8.2903e-02, -1.7831e-02,  2.0515e-02, -3.9681e-02,  8.5650e-02,
          -4.3124e-02,  4.8030e-02,  8.2660e-02,  1.0859e-02,  9.7798e-02,
           6.1700e-02, -1.0957e-01,  1.1610e-02, -1.8312e-03,  1.1627e-01,
          -2.7741e-02, -1.6661e-02, -9.2794e-02, -4.1570e-02, -3.3134e-02,
          -1.1843e-01, -9.2639e-02,  8.6162e-02, -6.8464e-02, -5.7484e-02,
           1.0339e-01,  1.2372e-01,  3.2915e-02, -7.2578e-02, -6.4834e-02,
           1.1661e-01, -9.2767e-02,  2.5383e-02, -8.2752e-03, -3.4907e-02,
          -2.4611e-02, -2.8419e-02,  1.3093e-02,  6.6807e-02],
         [ 7.6618e-02, -1.2244e-01, -9.9313e-02, -8.1460e-02, -1.1616e-01,
          -8.2340e-02,  6.4932e-02,  8.3818e-02,  2.6146e-02,  1.1033e-01,
           2.9060e-03,  9.7646e-02,  1.0143e-01, -3.5500e-02,  1.6882e-02,
          -9.3788e-02,  2.4202e-02,  4.3673e-02, -5.4936e-02, -1.2184e-01,
          -1.0239e-02,  8.4310e-03, -1.1220e-01,  3.7968e-02,  1.8271e-02,
          -7.3331e-02, -7.7548e-02, -9.8082e-02, -8.3679e-02, -7.7123e-02,
           1.0157e-02, -1.0397e-01, -6.1334e-02,  1.0436e-01,  7.8021e-03,
           1.0701e-01, -2.1185e-02,  1.7208e-02,  9.6473e-02,  5.7033e-02,
           1.2889e-02,  7.9155e-02, -6.1489e-02, -2.1237e-02, -5.7642e-03,
           3.5762e-02,  9.8419e-02,  8.7373e-03, -5.7980e-02, -1.1489e-01,
           2.3542e-02, -8.8002e-02, -8.0419e-02, -9.7030e-02, -5.8966e-02,
           3.2272e-02, -3.0152e-02, -1.1044e-01,  6.3835e-02,  2.6988e-02,
          -1.1536e-02,  6.0504e-02, -6.9501e-02, -8.7866e-02],
         [ 8.9527e-03,  1.2219e-01, -2.2423e-02,  6.2937e-02,  3.4926e-02,
          -6.0279e-02, -4.9772e-02,  5.8363e-02, -6.7702e-03,  1.2430e-01,
          -4.9421e-03,  1.8840e-02,  1.6720e-02,  2.8279e-02, -7.0864e-02,
          -8.2478e-02, -7.4841e-02, -5.4919e-02, -3.6508e-02,  5.4104e-02,
           4.0075e-02,  1.1122e-01, -1.4245e-02, -9.0171e-02, -5.0040e-02,
          -4.5003e-04,  7.3574e-02, -4.6529e-02,  7.8002e-02, -1.2174e-02,
          -1.2302e-01,  2.9685e-02, -1.9901e-02,  8.8674e-02,  6.8284e-02,
          -7.2852e-02,  3.7474e-03,  2.8272e-03, -3.4050e-03, -2.4991e-02,
          -6.2578e-02,  3.5232e-02,  3.5183e-02,  2.1782e-03,  9.7185e-02,
           1.0385e-01,  1.0205e-01, -3.1673e-02,  2.2924e-02,  1.0860e-01,
           6.1248e-02,  5.5145e-02,  1.1735e-02,  3.4025e-02,  1.3389e-02,
          -1.9420e-02,  1.1994e-01,  5.6238e-02, -2.1556e-02,  5.3447e-02,
          -2.7995e-02,  8.0489e-02, -9.8868e-02,  8.2506e-02],
         [ 8.4675e-02,  4.2846e-02,  1.1050e-01, -7.8934e-02,  3.9100e-02,
           5.0324e-02, -4.6451e-02, -8.7531e-02, -5.4424e-02,  2.8045e-02,
          -6.7171e-02,  8.9513e-02, -1.8526e-02,  1.1380e-01, -2.6486e-02,
           7.2474e-02,  3.6060e-03,  8.6258e-02,  1.0897e-01, -8.9708e-02,
          -5.5331e-02, -5.0644e-02,  1.0393e-01,  2.0892e-02, -6.3174e-02,
          -8.4175e-02,  9.2779e-02, -3.5423e-02,  5.3443e-03,  6.8273e-02,
          -9.9062e-02,  1.1734e-01,  8.5260e-02,  3.9756e-02, -1.7330e-02,
          -4.7684e-02, -6.7934e-02,  1.1131e-01, -9.9632e-02,  2.2051e-02,
           4.1641e-02,  3.1948e-02,  1.2957e-02, -5.5691e-02,  4.4362e-02,
           8.2845e-02, -7.9093e-02,  5.9158e-02, -5.9455e-02, -3.4768e-02,
           1.2223e-01,  6.8441e-02,  7.1273e-02, -2.7162e-02, -3.8217e-02,
           5.7713e-02,  1.9755e-02,  2.1536e-02,  1.2051e-01, -2.3186e-02,
           1.1316e-01, -9.0384e-02,  1.1155e-01, -3.9118e-02],
         [-6.4486e-02,  9.1205e-02,  7.2617e-02,  8.9787e-02,  1.0597e-01,
          -5.1348e-02, -5.4956e-02, -1.2248e-01, -6.8996e-02,  1.2337e-02,
           1.1319e-01,  6.4906e-02, -1.0805e-01, -1.0657e-02,  9.0965e-02,
          -1.0339e-01,  2.2327e-02, -9.9746e-02,  9.2759e-02, -5.2279e-03,
          -4.7267e-02, -1.1810e-01,  6.9174e-02, -6.4917e-02, -9.5603e-02,
           7.3767e-02,  8.3071e-03,  1.2049e-01,  2.3683e-02,  1.0024e-01,
          -8.9136e-02,  1.1009e-01,  7.5326e-03,  3.7416e-02, -1.1955e-01,
           9.0777e-02,  1.0708e-01, -9.8210e-02, -6.0032e-02, -2.0963e-02,
           1.1606e-01,  6.0629e-02, -7.1133e-02,  1.0901e-01,  1.4987e-03,
           5.2123e-02,  4.6099e-02, -8.1967e-04,  1.3331e-02,  6.5132e-02,
          -4.0445e-02,  9.9031e-02,  1.2998e-02,  1.2258e-01,  9.7593e-02,
           4.7445e-02, -2.2570e-03,  3.6439e-02, -1.4321e-02,  7.3606e-02,
          -7.6002e-02, -7.6155e-02, -7.0826e-02, -9.1068e-02],
         [ 6.4219e-02,  5.5464e-03,  1.1814e-01,  5.4553e-02,  7.9101e-03,
           7.8380e-02,  7.8663e-02, -9.0881e-02, -1.1492e-01,  4.3437e-03,
          -5.2576e-02,  5.6029e-03,  1.2476e-01, -1.7814e-02,  6.6570e-02,
           8.4483e-02, -2.8237e-02,  1.0053e-01,  3.7056e-02,  4.4022e-02,
          -1.0409e-03, -7.9568e-02,  5.3468e-03, -6.1588e-02,  1.0463e-01,
           8.8804e-02,  1.0344e-01,  6.9672e-02, -8.9021e-02, -6.2312e-02,
           4.7477e-02,  1.0262e-01, -8.9992e-02,  9.1983e-02, -5.2173e-02,
          -1.2268e-01, -1.0120e-01, -3.1153e-02,  8.3368e-02,  2.3747e-02,
           2.1874e-02, -4.5030e-02, -1.1300e-01,  5.1506e-02,  8.2110e-03,
           4.4078e-02,  1.1462e-02,  1.1731e-01,  1.0057e-01,  2.3584e-03,
          -4.9075e-02,  7.3112e-02,  3.2957e-03,  3.0769e-02, -1.1413e-01,
           7.7731e-02, -1.1153e-01, -3.7248e-02,  1.0061e-01,  1.1496e-01,
           1.1802e-01,  3.6183e-02,  3.5370e-02, -1.2370e-01]],
        requires_grad=True), Parameter containing:
 tensor([ 0.1159, -0.0094, -0.0804,  0.0886, -0.0898, -0.0913,  0.0249,  0.0978,
         -0.0063,  0.0555], requires_grad=True))
```

```python
# Case 2
model = nn.Linear(64, 10)
model.weight = nn.Parameter(torch.zeros(10, 64))
model.bias = nn.Parameter(torch.zeros(10,))

z = model(x)
logit = torch.sigmoid(z)

optimizer = torch.optim.Adam(model.parameters())
criterion = nn.BCELoss()
y_true = torch.zeros(logit.size())
bsz = y_true.size(0)
y_true[torch.arange(bsz), torch.LongTensor(bsz).random_(0,2)] = 1

loss = criterion(logit, y_true)
loss.backward()
optimizer.step()

model.weight, model.bias
```
```
(Parameter containing:
 tensor([[ 0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
           0.0010, -0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010,  0.0010, -0.0010, -0.0010,  0.0010, -0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,  0.0010, -0.0010,
           0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010],
         [-0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010, -0.0010,
           0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,
           0.0010,  0.0010, -0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,
          -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010,
           0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,  0.0010,
          -0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010]],
        requires_grad=True), Parameter containing:
 tensor([ 0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,
         -0.0010, -0.0010], requires_grad=True))
```
